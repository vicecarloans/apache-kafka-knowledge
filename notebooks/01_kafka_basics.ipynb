{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Kafka: The Father of Distributed Messaging Platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kafka is a distributed streaming platform which was written by Franz Kafka who was a famous author in the 19th century. He wrote a book called \"The Metamorphosis\".\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "<img src=\"../images/franz-kafka.jpeg\" alt=\"Franz Kafka\" style=\"max-height: 500px;\">\n",
    "\n",
    "<img src=\"../images/metamorphosis.jpg\" alt=\"Metamorphosis\" style=\"max-height: 500px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Just kidding!\n",
    "\n",
    "It was created by Linkedin. They named it after the famous author Franz Kafka as it is a system that optimizes for \"writing\"\n",
    "\n",
    "Kafka is an influence of other distributed streaming platforms such as AWS Kinesis and Google Cloud Pub/Sub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kafka Architecture at Coinsquare\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/apache-kafka.png\" alt=\"Kafka Architecture\" style=\"height: 85%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kafka Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- **Topics**: a queue where messages will be published to\n",
    "\n",
    "- **Partitions**: enables parallelism in Kafka. One topic can have multiple partitions\n",
    "\n",
    "- **Apache Avro**: Schema Registry. Used to defined a typed-strong message and validate it before sending out. Messages can be serialized in binary\n",
    "\n",
    "- **Producer**: Publish messages\n",
    "\n",
    "- **Messages**: Data being published by producer (Duh) which can then be consumed in downstream services. Messages are immutable and can only be changed by publishing a new message.\n",
    "\n",
    "- **Offset**: Messages are arrived in order and their position is tracked by an offset.\n",
    "\n",
    "- **Consumer**: End client that consumes messages from a partition\n",
    "\n",
    "- **Consumer Group**: Kafka assigns the partitions of a topic to the consumers in a group so that each partition is assigned to one consumer in the group. **Two consumers in the same group will not receive the same message**. However, **Two consumers in different groups can receive the same message**. Distribution rules are defined as:\n",
    "    - If consumers < partitions: Some consumers will handle multiple partitions\n",
    "    - If consumers = partitions: Each consumer gets one partition\n",
    "    - If consumers > partitions: Some consumers will be idle\n",
    "    - When a consumer fails, the partitions assigned to it will be reassigned to another consumer in the same group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **KRaft or ZooKeeper**: Manage Cluster (CPU, Memory, Topic, Partition Balancing). ZooKeeper is a dependency for old version of Kafka whereas KRaft is enabled natively in the new version 2.8+\n",
    "\n",
    "- **Broker**: A Kafka server node that is responsible for multiple partitions of multiple topics. A broker can be elected to be a leader that is responsible for replicating data across partitions\n",
    "\n",
    "- **Exactly-once Delivery**: Kafkaâ€™s Transactional API can ensure each message to be processed exactly once.\n",
    "\n",
    "- **At-least-once Delivery**: Messages never lost but can be processed more than once. This happens if offsets are failed to committed (due to network, consumer crash)\n",
    "\n",
    "- **Dead Letter Queue**: Kafka does not provide built-in DLQ like services such as PubSub or SQS. You have to implement a topic and a library to send to dead letter queue.\n",
    "\n",
    "- **Retry Strategy**: Kafka does not provide built-in retry mechanism per topic. You have to either do that at Consumer level or create a bunch of retry topics with different delays\n",
    "\n",
    "- **Kafka Connect**: Kafka connector to connect to external systems such as databases, S3, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Broadcast messages to multiple consumers (One publisher multiple consumers)\n",
    "\n",
    "```\n",
    "cd samples/broadcast\n",
    "python producer.py \n",
    "# in another terminal\n",
    "python consumer.py foo\n",
    "python consumer.py bar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Chat Room\n",
    "\n",
    "```\n",
    "cd samples/chat-room\n",
    "python chat_client.py huy\n",
    "python chat_client.py karida\n",
    "python chat_client.py lily\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gotchas and Best Practices\n",
    "\n",
    "<img src=\"../images/kafka-best-practices.jpg\" alt=\"Kafka Best Practices\" style=\"height: 65%; margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gotchas\n",
    "\n",
    "- A message usually has maximum size of 1MB. You can bump this number up but in can affect performance or OutOfMemoryErrors. \n",
    "\n",
    "- Sending large messages such as PDF should use an interim Storage System and only be sent as reference.\n",
    "\n",
    "- Donâ€™t publish plain message. Use a schema registry such as Apache Avro\n",
    "\n",
    "- acks: set to 0/1/all to ensure durability. With 0, the producer doesnâ€™t wait for acknowledgement. With 1, the producer waits for leader to acknowledge. With all, the producer wait for leader and all replicas to ack\n",
    "\n",
    "- Consumer must commit offset after processing using commitSync() or commitAsync()\n",
    "\n",
    "- Start with num partitions = num consumers and increase the size accordingly. Too few can cause throughput issue while too many can cause slowness in rebalancing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kafka sometimes rebalances consumers â†”ï¸Ž partitions. Rebalancing involves consumers stop processing data. Use Cooperative Sticky Assignor introduced in 2.4+ to incrementally reassigning partitions.\n",
    "\n",
    "- Log Retention should be appropriate and based off how long you want to keep your messages\n",
    "\n",
    "- Ensure you have a database for each consumer to query for idempotency to avoid processing messages twice if you donâ€™t use Transactional API.\n",
    "\n",
    "- Have Dead Letter Queue and appropriate monitoring on DLQ\n",
    "\n",
    "- Have Retry Strategy in place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Best Practices\n",
    "\n",
    "- **Topic Naming**: `{environment}.{service}.{topic}`\n",
    "    - Use lowercase\n",
    "    - Separate by dot\n",
    "    - Examples: `dev.order.created`, `prod.order.created`\n",
    "- **Partitioning**: \n",
    "    - Start small (3 partitions per topic)\n",
    "    - Avoid too many partitions (can impact performance and leader election)\n",
    "    - Use partition keys when publishing messages to distribute messages evenly across partitions\n",
    "- **Schema Management**:\n",
    "    - Use Avro to define a typed-strong message and validate it before sending out\n",
    "    - Use Avro to serialize and deserialize messages\n",
    "    - Plan for backward/forward compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Message Content**:\n",
    "  - Include metadata (timestamp, source, version)\n",
    "  - Keep messages reasonably sized (< 1MB)\n",
    "  - Use compression for large messages\n",
    "  - Include correlation IDs for tracking\n",
    "\n",
    "- **Monitoring**:\n",
    "  - Monitor broker metrics (CPU, memory, disk)\n",
    "  - Track consumer lag which causes by consumers cannot keep up with the producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notable Mentions\n",
    "\n",
    "- **Kafka Streams**: A spin-off library that is lightweight and *in Java* ðŸ¤¢ for stream processing. Think of it as ETL but with a T only \n",
    "\n",
    "- **Kafka Connect**: A wrapper sitting in between Kafka and external systems such as datalakes, S3, etc.\n",
    "\n",
    "- **Kafka Streams** and **Kafka Connect** usually go hand in hand. where Connect is used to load data into Kafka and Streams is used to process data in Kafka. Check out this example from [Confluent](https://www.confluent.io/blog/hello-world-kafka-connect-kafka-streams/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
